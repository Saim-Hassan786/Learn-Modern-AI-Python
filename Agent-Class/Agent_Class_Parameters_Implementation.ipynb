{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzeQyQEWIBMfbV46QL59sN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saim-Hassan786/Learn-Modern-AI-Python/blob/main/Agent-Class/Agent_Class_Parameters_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenAI Agents SDK\n",
        "OpenAI Agents SDK as it name implies is a Software Development Kit or a FrameWork by OpenAI that helps us make Autonomous AI Agentic Apps that are capable of thinking, understanding and acting autonomously to resolve a user query based on its instructions.\n",
        "\n",
        "## Features of OpenAI Agents SDk\n",
        "1. Python First Aproach .\n",
        "2. Agents : LLM configured with tools and instructions.\n",
        "3. Handoffs : Delegating task from one Agent to Another.\n",
        "4. Guardrails : For Input And Output Validation.\n",
        "5. Tracing : For Logging and Debugging.\n",
        "6. FunctionTools : Adding to the functionality of the Agent\n",
        "7. AgentLoop : A loop that handles all the processing until the request in done.\n"
      ],
      "metadata": {
        "id": "Rk117KGfT_1k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW2j-jEZSOaQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42611ee3-6f6a-47c9-accc-d29df6c001ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m130.6/130.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Installing the SDK\n",
        "!pip install -Uq openai-agents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For running event loop\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "J0GarRCRemG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pre requisites SetUp\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY= userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "from agents import set_default_openai_api,set_default_openai_client,set_tracing_disabled\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "external_client = AsyncOpenAI(\n",
        "    base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        "    api_key = GOOGLE_API_KEY\n",
        ")\n",
        "set_default_openai_client(external_client)\n",
        "set_default_openai_api(\"chat_completions\")\n",
        "set_tracing_disabled(True)"
      ],
      "metadata": {
        "id": "P6_-ns8JYQX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Agent Execution Using OpenAI Agents SDK\n",
        "from agents import Agent,Runner\n",
        "agent = Agent(\n",
        "    name=\"Simple Assistant ChatBot\",\n",
        "    instructions= \"You are a simple assistant that helps user with their queries\",\n",
        "    model = \"gemini-2.5-flash\"\n",
        ")\n",
        "result = await Runner.run(\n",
        "    starting_agent=agent,\n",
        "    input=\"What are the capitals of France and Germany?\",\n",
        ")\n",
        "result.final_output"
      ],
      "metadata": {
        "id": "EunmGdKuT2bw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5e51285e-97f8-4b22-f99f-910f516d4c77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The capital of France is **Paris**.\\nThe capital of Germany is **Berlin**.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AGENT CLASS\n",
        "Agent is regarded as a LLM configured with instructions, tools, guardrails , handoffs and many more.\n",
        "Agent is a data class and it contains following parameters :\n",
        "\n",
        "1. **name** : str = name of the Agent\n",
        "2. **instructions** : *str | Callable [Context:RunContextWrapper,agent:Agent],MaybeAwaitable[str]] | None* = that acts as a system prompt for the LLM when the agent is invoked\n",
        "3. **model** : str | Model = the model which is used by the LLM when agent is invoked\n",
        "4. **tools** : list[Tool] = external tools that help enhance the capabilities of an agent\n",
        "5. **handoffs** : list[Agent] | list[Handoffs] = the list of task specific agents that the current agent can delegate to\n",
        "6. **handoff_description** : str = a description for the agent if it to be act as a handoff for performing a delegated task\n",
        "7. **mcp_servers** : list[MCPServer] = list of MCP hosted tools that can aid in the agent capabilities\n",
        "8. **mcp_config** : MCPConfig = configurations of MCP server tools\n",
        "9. **model_settings** : MODELSETTINGS = configuration settings for the model of LLM that we have chosen fro our agent\n",
        "10. **hooks** : AgentHooks = callback functions on different lifecycle events of an agent\n",
        "11. **input_guardrails** : list[InputGuardrails] = list of checks that run in parallel with agent execution to filter the input coming to agent and stop its execution on specific scenarios\n",
        "12. **output_guardrails** : list[OutputGuardrails] = list of checks that run on the final output of the agent when it has generated the final response to control the final result generation on demand\n",
        "13. **tool_use_behaviour** : Literal[\"run_llm_again\",\"stop_on_first_tool\"] | StopAtTools | ToolsToFinalOutputFunction | default = \"run_llm_again\" = a set of behaviour controlling instructions for a Runner Loop regarding the LLM usage.\n",
        "14. **reset_tool_choice** : bool | default = True = to prevent the loop for going into infinite loop so it is set to TRUE to prevent infinit tool usage\n",
        "15. **output_type** : type[Any] | AgentOutputSchemaBase | None (if None then default is str) = can define the output type for the final response."
      ],
      "metadata": {
        "id": "0B6muv3VTsFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# name"
      ],
      "metadata": {
        "id": "hdcpf_MYxkA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To initiate an Agent the required parameter is \"name\"\n",
        "from agents import Agent\n",
        "agent = Agent(\n",
        "    name = \"Assistant\"\n",
        ")"
      ],
      "metadata": {
        "id": "1lrilvSCTjQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# instructions"
      ],
      "metadata": {
        "id": "wTSTYiyTC1bZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# static string instructions\n",
        "from agents import Agent,Runner\n",
        "agent = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    instructions= \"You are a helpful assistant.\",\n",
        "    model = \"gemini-2.5-flash\"\n",
        ")\n",
        "result = Runner.run_sync(\n",
        "    starting_agent=agent,\n",
        "    input=\"What are the capitals of France and Germany?\",\n",
        ")\n",
        "print(f\"Static Instructions : {result.final_output}\")\n",
        "\n",
        "# dynamic instructions\n",
        "from pydantic import BaseModel\n",
        "from agents import Agent,Runner,RunContextWrapper\n",
        "\n",
        "class Userdata(BaseModel):\n",
        "  name : str\n",
        "  age : int\n",
        "\n",
        "async def dynamic_instructions(ctx : RunContextWrapper[Userdata],agent : Agent)->str:\n",
        "  return f\"You are a helpful Assistant for {ctx.context.name} so always respond his queries by confronting his name first\"\n",
        "\n",
        "agent_instructions = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    instructions= dynamic_instructions,\n",
        "    model = \"gemini-2.5-flash\"\n",
        ")\n",
        "result_instructions = Runner.run_sync(\n",
        "    starting_agent=agent_instructions,\n",
        "    input=\"What are the capitals of France and Germany?\",\n",
        "    context = Userdata(name=\"Saim\",age=20)\n",
        ")\n",
        "print(f\"Dynamic Instructions : {result_instructions.final_output}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZqadLcWC43v",
        "outputId": "9572707a-b09c-4289-9ba5-9251591c25ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Static Instructions : The capital of France is **Paris**, and the capital of Germany is **Berlin**.\n",
            "Dynamic Instructions : Saim, the capital of France is Paris and the capital of Germany is Berlin.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model"
      ],
      "metadata": {
        "id": "Snfrbef3SbmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model parameter\n",
        "from agents import Agent, Runner\n",
        "\n",
        "agent_with_model_1 = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    instructions= \"You are a helpful assistant.\",\n",
        "    model = \"gemini-2.5-flash\"\n",
        ")\n",
        "result_with_model_1 = Runner.run_sync(\n",
        "    starting_agent=agent_with_model_1,\n",
        "    input=\"What is cloud computing in 100 words?\",\n",
        ")\n",
        "print(f\"Model 1 : {result_with_model_1.final_output}\")\n",
        "print(\"=======\" * 20)\n",
        "\n",
        "agent_with_model_2 = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    instructions= \"You are a helpful assistant.\",\n",
        "    model = \"gemini-2.5-pro\"\n",
        ")\n",
        "result_with_model_2 = Runner.run_sync(\n",
        "    starting_agent=agent_with_model_2,\n",
        "    input=\"What is agentic ai in 100 words?\",\n",
        ")\n",
        "print(f\"Model 2 : {result_with_model_2.final_output}\")\n",
        "print(\"=======\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4HkRrDQSeqU",
        "outputId": "514b74a3-89f8-4b86-93bf-7370a08de8e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1 : Cloud computing is the delivery of on-demand computing services‚Äîincluding servers, storage, databases, networking, software, and analytics‚Äîover the Internet (\"the cloud\"). Instead of owning and maintaining your own computing infrastructure, you access these services from a third-party cloud provider (e.g., AWS, Azure).\n",
            "\n",
            "Key benefits include pay-as-you-go pricing, elasticity (scaling resources up or down quickly), and high availability. This model reduces operational costs and increases flexibility, allowing businesses and individuals to leverage powerful technology without the burden of maintaining physical hardware. It democratizes IT resources, enabling faster innovation and scalability for various needs.\n",
            "============================================================================================================================================\n",
            "Model 2 : Agentic AI represents a shift from passive, reactive systems to proactive, autonomous ones. Instead of just responding to a prompt, an AI agent is given a high-level goal.\n",
            "\n",
            "It then independently creates a plan, breaks it down into executable steps, and uses tools (like a web browser or code interpreter) to carry out those steps.\n",
            "\n",
            "Crucially, the agent observes the results of its actions, learns from successes and failures, and adapts its strategy to achieve the final objective with minimal human intervention. It‚Äôs AI that doesn't just answer, but *acts* to complete complex tasks.\n",
            "============================================================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tools"
      ],
      "metadata": {
        "id": "UVM_umpcTXPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tools list using @function_tool\n",
        "from agents import Agent,Runner,function_tool\n",
        "from agents.extensions.visualization import draw_graph\n",
        "\n",
        "@function_tool\n",
        "def multiply(a:int,b:int) -> int:\n",
        "  \"\"\"Multiplies two numbers\"\"\"\n",
        "  return a * b\n",
        "\n",
        "@function_tool\n",
        "def division(c:int,d:int) -> int | str :\n",
        "  \"\"\"Divides two numbers\"\"\"\n",
        "  try :\n",
        "    return c / d\n",
        "  except ZeroDivisionError:\n",
        "    return \"Cannot divide by zero\"\n",
        "\n",
        "agent_with_tools = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    instructions= \"You are a helpful assistant.\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    tools = [multiply,division]\n",
        ")\n",
        "result_with_tools_1 = Runner.run_sync(\n",
        "    starting_agent=agent_with_tools,\n",
        "    input=\"Multiply 5 and 10\",\n",
        ")\n",
        "print(f\"Tools 1 : {result_with_tools_1.final_output}\")\n",
        "print(\"=======\" * 20)\n",
        "\n",
        "result_with_tools_2 = Runner.run_sync(\n",
        "    starting_agent=agent_with_tools,\n",
        "    input=\"Divide 10 by 0\",\n",
        ")\n",
        "print(f\"Tools 2 : {result_with_tools_2.final_output}\")\n",
        "print(\"=======\" * 20)\n",
        "\n",
        "# For visualization\n",
        "draw_graph(agent_with_tools)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "_WaTieu9TY8J",
        "outputId": "37ae76bc-f896-431b-dcd1-3dbd892d0af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tools 1 : The result is 50.\n",
            "============================================================================================================================================\n",
            "Tools 2 : I cannot divide by zero. Please provide a different number to divide by.\n",
            "============================================================================================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: G Pages: 1 -->\n<svg width=\"318pt\" height=\"203pt\"\n viewBox=\"0.00 0.00 317.63 203.05\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 199.05)\">\n<title>G</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-199.05 313.63,-199.05 313.63,4 -4,4\"/>\n<!-- __start__ -->\n<g id=\"node1\" class=\"node\">\n<title>__start__</title>\n<ellipse fill=\"lightblue\" stroke=\"black\" cx=\"159.79\" cy=\"-178.79\" rx=\"51.74\" ry=\"16.03\"/>\n<text text-anchor=\"middle\" x=\"159.79\" y=\"-175.09\" font-family=\"Arial\" font-size=\"14.00\">__start__</text>\n</g>\n<!-- Assistant Agent -->\n<g id=\"node3\" class=\"node\">\n<title>Assistant Agent</title>\n<polygon fill=\"lightyellow\" stroke=\"black\" points=\"214.79,-126.53 104.79,-126.53 104.79,-68.53 214.79,-68.53 214.79,-126.53\"/>\n<text text-anchor=\"middle\" x=\"159.79\" y=\"-93.83\" font-family=\"Arial\" font-size=\"14.00\">Assistant Agent</text>\n</g>\n<!-- __start__&#45;&gt;Assistant Agent -->\n<g id=\"edge1\" class=\"edge\">\n<title>__start__&#45;&gt;Assistant Agent</title>\n<path fill=\"none\" stroke=\"black\" stroke-width=\"1.5\" d=\"M159.79,-162.51C159.79,-155.12 159.79,-145.89 159.79,-136.76\"/>\n<polygon fill=\"black\" stroke=\"black\" stroke-width=\"1.5\" points=\"163.29,-136.6 159.79,-126.6 156.29,-136.6 163.29,-136.6\"/>\n</g>\n<!-- __end__ -->\n<g id=\"node2\" class=\"node\">\n<title>__end__</title>\n<ellipse fill=\"lightblue\" stroke=\"black\" cx=\"48.79\" cy=\"-16.26\" rx=\"48.58\" ry=\"16.03\"/>\n<text text-anchor=\"middle\" x=\"48.79\" y=\"-12.56\" font-family=\"Arial\" font-size=\"14.00\">__end__</text>\n</g>\n<!-- Assistant Agent&#45;&gt;__end__ -->\n<g id=\"edge6\" class=\"edge\">\n<title>Assistant Agent&#45;&gt;__end__</title>\n<path fill=\"none\" stroke=\"black\" stroke-width=\"1.5\" d=\"M120.34,-68.36C105.93,-58.06 89.93,-46.64 76.73,-37.21\"/>\n<polygon fill=\"black\" stroke=\"black\" stroke-width=\"1.5\" points=\"78.67,-34.3 68.5,-31.34 74.6,-40 78.67,-34.3\"/>\n</g>\n<!-- multiply -->\n<g id=\"node4\" class=\"node\">\n<title>multiply</title>\n<ellipse fill=\"lightgreen\" stroke=\"black\" cx=\"159.79\" cy=\"-16.26\" rx=\"44.6\" ry=\"16.03\"/>\n<text text-anchor=\"middle\" x=\"159.79\" y=\"-12.56\" font-family=\"Arial\" font-size=\"14.00\">multiply</text>\n</g>\n<!-- Assistant Agent&#45;&gt;multiply -->\n<g id=\"edge2\" class=\"edge\">\n<title>Assistant Agent&#45;&gt;multiply</title>\n<path fill=\"none\" stroke=\"black\" stroke-width=\"1.5\" stroke-dasharray=\"1,5\" d=\"M153.3,-68.36C152.91,-59.94 152.96,-50.77 153.45,-42.55\"/>\n<polygon fill=\"black\" stroke=\"black\" stroke-width=\"1.5\" points=\"156.95,-42.71 154.35,-32.44 149.98,-42.09 156.95,-42.71\"/>\n</g>\n<!-- division -->\n<g id=\"node5\" class=\"node\">\n<title>division</title>\n<ellipse fill=\"lightgreen\" stroke=\"black\" cx=\"265.79\" cy=\"-16.26\" rx=\"43.68\" ry=\"16.03\"/>\n<text text-anchor=\"middle\" x=\"265.79\" y=\"-12.56\" font-family=\"Arial\" font-size=\"14.00\">division</text>\n</g>\n<!-- Assistant Agent&#45;&gt;division -->\n<g id=\"edge4\" class=\"edge\">\n<title>Assistant Agent&#45;&gt;division</title>\n<path fill=\"none\" stroke=\"black\" stroke-width=\"1.5\" stroke-dasharray=\"1,5\" d=\"M190.97,-68.36C204.86,-57.6 221.12,-45.6 234.98,-35.94\"/>\n<polygon fill=\"black\" stroke=\"black\" stroke-width=\"1.5\" points=\"236.99,-38.81 243.26,-30.26 233.03,-33.04 236.99,-38.81\"/>\n</g>\n<!-- multiply&#45;&gt;Assistant Agent -->\n<g id=\"edge3\" class=\"edge\">\n<title>multiply&#45;&gt;Assistant Agent</title>\n<path fill=\"none\" stroke=\"black\" stroke-width=\"1.5\" stroke-dasharray=\"1,5\" d=\"M165.24,-32.44C166.15,-39.82 166.58,-49.05 166.54,-58.18\"/>\n<polygon fill=\"black\" stroke=\"black\" stroke-width=\"1.5\" points=\"163.03,-58.27 166.28,-68.36 170.03,-58.45 163.03,-58.27\"/>\n</g>\n<!-- division&#45;&gt;Assistant Agent -->\n<g id=\"edge5\" class=\"edge\">\n<title>division&#45;&gt;Assistant Agent</title>\n<path fill=\"none\" stroke=\"black\" stroke-width=\"1.5\" stroke-dasharray=\"1,5\" d=\"M251.75,-31.77C241.34,-40.39 226.65,-51.71 212.04,-62.46\"/>\n<polygon fill=\"black\" stroke=\"black\" stroke-width=\"1.5\" points=\"209.97,-59.64 203.95,-68.36 214.09,-65.29 209.97,-59.64\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.sources.Source at 0x7b5e8e4b14d0>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# handoffs"
      ],
      "metadata": {
        "id": "3GEI-qX5Uo9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# handoffs\n",
        "from agents import Agent,Runner\n",
        "from agents.extensions.visualization import draw_graph\n",
        "\n",
        "spanish_agent = Agent(\n",
        "    name = \"Spanish Agent\",\n",
        "    instructions= \"You are a helpful assistant that help in translation into spanish\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    handoff_description=\"Spanish Agent Specialized in Spanish Translation\"\n",
        ")\n",
        "german_agent = Agent(\n",
        "    name = \"German Agent\",\n",
        "    instructions= \"You are a helpful assistant that help in translation into german\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    handoff_description=\"German Agent Specialized in German Translation\"\n",
        ")\n",
        "french_agent = Agent(\n",
        "    name = \"French Agent\",\n",
        "    instructions= \"You are a helpful assistant that help in translation into french\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    handoff_description=\"French Agent Specialized in French Translation\"\n",
        ")\n",
        "\n",
        "agent_with_handoffs = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    instructions= \"You are a helpful assistant.\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    handoffs = [spanish_agent,german_agent,french_agent]\n",
        ")\n",
        "result_with_handoffs_1 = Runner.run_sync(\n",
        "    starting_agent=agent_with_handoffs,\n",
        "    input=\"Translate into Spanish this sentence 'My name is Saim Hassan'\",\n",
        ")\n",
        "print(f\"Handoff To Spanish Agent: {result_with_handoffs_1.final_output}\")\n",
        "print(\"=======\" * 20)\n",
        "\n",
        "result_with_handoffs_2 = Runner.run_sync(\n",
        "    starting_agent=agent_with_handoffs,\n",
        "    input=\"Translate into German this sentence 'My name is Saim Hassan'\",\n",
        ")\n",
        "print(f\"Handoff To German Agent: {result_with_handoffs_2.final_output}\")\n",
        "print(\"=======\" * 20)\n",
        "\n",
        "result_with_handoffs_3 = Runner.run_sync(\n",
        "    starting_agent=agent_with_handoffs,\n",
        "    input=\"Translate into French this sentence 'My name is Saim Hassan'\",\n",
        ")\n",
        "print(f\"Handoff To French Agent: {result_with_handoffs_3.final_output}\")\n",
        "print(\"=======\" * 20)\n",
        "\n",
        "# For Visualization of the agent execution\n",
        "draw_graph(agent_with_handoffs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "wPASlxY7UqyW",
        "outputId": "4fe8f88b-79ca-4005-f46b-ce4c8fd9d5fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Handoff To Spanish Agent: \"Me llamo Saim Hassan.\"\n",
            "============================================================================================================================================\n",
            "Handoff To German Agent: \"Mein Name ist Saim Hassan.\"\n",
            "============================================================================================================================================\n",
            "Handoff To French Agent: Mon nom est Saim Hassan.\n",
            "============================================================================================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: G Pages: 1 -->\n<svg width=\"368pt\" height=\"297pt\"\n viewBox=\"0.00 0.00 368.00 297.05\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 293.05)\">\n<title>G</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-293.05 364,-293.05 364,4 -4,4\"/>\n<!-- __start__ -->\n<g id=\"node1\" class=\"node\">\n<title>__start__</title>\n<ellipse fill=\"lightblue\" stroke=\"black\" cx=\"180\" cy=\"-272.79\" rx=\"51.74\" ry=\"16.03\"/>\n<text text-anchor=\"middle\" x=\"180\" y=\"-269.09\" font-family=\"Arial\" font-size=\"14.00\">__start__</text>\n</g>\n<!-- Assistant Agent -->\n<g id=\"node3\" class=\"node\">\n<title>Assistant Agent</title>\n<polygon fill=\"lightyellow\" stroke=\"black\" points=\"235,-220.53 125,-220.53 125,-162.53 235,-162.53 235,-220.53\"/>\n<text text-anchor=\"middle\" x=\"180\" y=\"-187.83\" font-family=\"Arial\" font-size=\"14.00\">Assistant Agent</text>\n</g>\n<!-- __start__&#45;&gt;Assistant Agent -->\n<g id=\"edge1\" class=\"edge\">\n<title>__start__&#45;&gt;Assistant Agent</title>\n<path fill=\"none\" stroke=\"black\" stroke-width=\"1.5\" d=\"M180,-256.51C180,-249.12 180,-239.89 180,-230.76\"/>\n<polygon fill=\"black\" stroke=\"black\" stroke-width=\"1.5\" points=\"183.5,-230.6 180,-220.6 176.5,-230.6 183.5,-230.6\"/>\n</g>\n<!-- __end__ -->\n<g id=\"node2\" class=\"node\">\n<title>__end__</title>\n<ellipse fill=\"lightblue\" stroke=\"black\" cx=\"180\" cy=\"-16.26\" rx=\"48.58\" ry=\"16.03\"/>\n<text text-anchor=\"middle\" x=\"180\" y=\"-12.56\" font-family=\"Arial\" font-size=\"14.00\">__end__</text>\n</g>\n<!-- Spanish Agent -->\n<g id=\"node4\" class=\"node\">\n<title>Spanish Agent</title>\n<path fill=\"none\" stroke=\"black\" d=\"M96,-126.53C96,-126.53 12,-126.53 12,-126.53 6,-126.53 0,-120.53 0,-114.53 0,-114.53 0,-80.53 0,-80.53 0,-74.53 6,-68.53 12,-68.53 12,-68.53 96,-68.53 96,-68.53 102,-68.53 108,-74.53 108,-80.53 108,-80.53 108,-114.53 108,-114.53 108,-120.53 102,-126.53 96,-126.53\"/>\n<text text-anchor=\"middle\" x=\"54\" y=\"-93.83\" font-family=\"Arial\" font-size=\"14.00\">Spanish Agent</text>\n</g>\n<!-- Assistant Agent&#45;&gt;Spanish Agent -->\n<g id=\"edge2\" class=\"edge\">\n<title>Assistant Agent&#45;&gt;Spanish Agent</title>\n<path fill=\"none\" stroke=\"black\" stroke-width=\"1.5\" d=\"M141.51,-162.42C128.69,-153.06 114.25,-142.52 100.87,-132.75\"/>\n<polygon fill=\"black\" stroke=\"black\" stroke-width=\"1.5\" points=\"102.83,-129.85 92.69,-126.77 98.7,-135.5 102.83,-129.85\"/>\n</g>\n<!-- German Agent -->\n<g id=\"node5\" class=\"node\">\n<title>German Agent</title>\n<path fill=\"none\" stroke=\"black\" d=\"M222,-126.53C222,-126.53 138,-126.53 138,-126.53 132,-126.53 126,-120.53 126,-114.53 126,-114.53 126,-80.53 126,-80.53 126,-74.53 132,-68.53 138,-68.53 138,-68.53 222,-68.53 222,-68.53 228,-68.53 234,-74.53 234,-80.53 234,-80.53 234,-114.53 234,-114.53 234,-120.53 228,-126.53 222,-126.53\"/>\n<text text-anchor=\"middle\" x=\"180\" y=\"-93.83\" font-family=\"Arial\" font-size=\"14.00\">German Agent</text>\n</g>\n<!-- Assistant Agent&#45;&gt;German Agent -->\n<g id=\"edge4\" class=\"edge\">\n<title>Assistant Agent&#45;&gt;German Agent</title>\n<path fill=\"none\" stroke=\"black\" stroke-width=\"1.5\" d=\"M180,-162.42C180,-154.38 180,-145.48 180,-136.94\"/>\n<polygon fill=\"black\" stroke=\"black\" stroke-width=\"1.5\" points=\"183.5,-136.77 180,-126.77 176.5,-136.77 183.5,-136.77\"/>\n</g>\n<!-- French Agent -->\n<g id=\"node6\" class=\"node\">\n<title>French Agent</title>\n<path fill=\"none\" stroke=\"black\" d=\"M348,-126.53C348,-126.53 264,-126.53 264,-126.53 258,-126.53 252,-120.53 252,-114.53 252,-114.53 252,-80.53 252,-80.53 252,-74.53 258,-68.53 264,-68.53 264,-68.53 348,-68.53 348,-68.53 354,-68.53 360,-74.53 360,-80.53 360,-80.53 360,-114.53 360,-114.53 360,-120.53 354,-126.53 348,-126.53\"/>\n<text text-anchor=\"middle\" x=\"306\" y=\"-93.83\" font-family=\"Arial\" font-size=\"14.00\">French Agent</text>\n</g>\n<!-- Assistant Agent&#45;&gt;French Agent -->\n<g id=\"edge6\" class=\"edge\">\n<title>Assistant Agent&#45;&gt;French Agent</title>\n<path fill=\"none\" stroke=\"black\" stroke-width=\"1.5\" d=\"M218.49,-162.42C231.31,-153.06 245.75,-142.52 259.13,-132.75\"/>\n<polygon fill=\"black\" stroke=\"black\" stroke-width=\"1.5\" points=\"261.3,-135.5 267.31,-126.77 257.17,-129.85 261.3,-135.5\"/>\n</g>\n<!-- Spanish Agent&#45;&gt;__end__ -->\n<g id=\"edge3\" class=\"edge\">\n<title>Spanish Agent&#45;&gt;__end__</title>\n<path fill=\"none\" stroke=\"black\" stroke-width=\"1.5\" d=\"M98.78,-68.36C115.69,-57.72 134.52,-45.88 149.78,-36.27\"/>\n<polygon fill=\"black\" stroke=\"black\" stroke-width=\"1.5\" points=\"151.72,-39.19 158.32,-30.9 147.99,-33.26 151.72,-39.19\"/>\n</g>\n<!-- German Agent&#45;&gt;__end__ -->\n<g id=\"edge5\" class=\"edge\">\n<title>German Agent&#45;&gt;__end__</title>\n<path fill=\"none\" stroke=\"black\" stroke-width=\"1.5\" d=\"M180,-68.36C180,-60.07 180,-51.04 180,-42.92\"/>\n<polygon fill=\"black\" stroke=\"black\" stroke-width=\"1.5\" points=\"183.5,-42.89 180,-32.89 176.5,-42.89 183.5,-42.89\"/>\n</g>\n<!-- French Agent&#45;&gt;__end__ -->\n<g id=\"edge7\" class=\"edge\">\n<title>French Agent&#45;&gt;__end__</title>\n<path fill=\"none\" stroke=\"black\" stroke-width=\"1.5\" d=\"M261.22,-68.36C244.31,-57.72 225.48,-45.88 210.22,-36.27\"/>\n<polygon fill=\"black\" stroke=\"black\" stroke-width=\"1.5\" points=\"212.01,-33.26 201.68,-30.9 208.28,-39.19 212.01,-33.26\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.sources.Source at 0x7b5e7e4ed450>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# handoff_description"
      ],
      "metadata": {
        "id": "6wGMYWM2XJcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent,Runner\n",
        "\n",
        "spanish_agent = Agent(\n",
        "    name = \"Spanish Agent\",\n",
        "    instructions= \"You are a helpful assistant that help in translation into spanish\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    handoff_description=\"Spanish Agent Specialized in Spanish Translation\"\n",
        ")\n",
        "german_agent = Agent(\n",
        "    name = \"German Agent\",\n",
        "    instructions= \"You are a helpful assistant that help in translation into german\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    handoff_description=\"German Agent Specialized in German Translation\"\n",
        ")\n",
        "\n",
        "agent_with_handoff_description = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    instructions= \"You are a helpful assistant.\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    handoffs = [spanish_agent,german_agent]\n",
        ")\n",
        "result_with_handoff_description_1 = Runner.run_sync(\n",
        "    starting_agent=agent_with_handoff_description,\n",
        "    input=\"Translate into Spanish this sentence 'Hi! How are you'\",\n",
        ")\n",
        "print(f\"Handoff To Spanish Agent With Handoffs Description: {result_with_handoff_description_1.final_output}\")\n",
        "print(\"=======\" * 20)\n",
        "\n",
        "result_with_handoff_description_2 = Runner.run_sync(\n",
        "    starting_agent=agent_with_handoff_description,\n",
        "    input=\"Translate into German this sentence 'Hi! How are you'\",\n",
        ")\n",
        "print(f\"Handoff To German Agent With Handoffs Description: {result_with_handoff_description_2.final_output}\")\n",
        "print(\"=======\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-qIVQmjXNaZ",
        "outputId": "086a377f-4400-49ab-c08f-91e8861be6d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Handoff To Spanish Agent With Handoffs Description: ¬°Hola! ¬øC√≥mo est√°s?\n",
            "============================================================================================================================================\n",
            "Handoff To German Agent With Handoffs Description: Here are a few options, depending on the level of formality:\n",
            "\n",
            "**Informal (most common for \"Hi!\"):**\n",
            "\n",
            "*   **Hallo! Wie geht es dir?** (Hello! How are you? - literally \"How goes it to you?\")\n",
            "*   **Hi! Wie geht's?** (Hi! How's it going? - a common, shorter form)\n",
            "\n",
            "**Formal (if you were speaking to someone you don't know well or someone in a position of respect):**\n",
            "\n",
            "*   **Hallo! Wie geht es Ihnen?** (Hello! How are you? - formal \"you\")\n",
            "============================================================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# mcp_servers\n",
        "**For MCP Server Tools Incorporation With Our Agent**"
      ],
      "metadata": {
        "id": "iz-Oiuf0ZXwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# mcp_config\n",
        "**To configure the setting for the MCP Server Tools incorporated with our agent.**"
      ],
      "metadata": {
        "id": "lx5gYe7eZkkd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# hooks"
      ],
      "metadata": {
        "id": "b9nlut8Lf5Kf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import time\n",
        "from agents import Agent,Runner,AgentHooks,RunContextWrapper,Tool,function_tool\n",
        "from agents.extensions.visualization import draw_graph\n",
        "import random\n",
        "import time\n",
        "\n",
        "class AgentWithHooks(AgentHooks):\n",
        "  def __init__(self,display_name:str):\n",
        "    self.event_counter = 0\n",
        "    self.display_name = display_name\n",
        "\n",
        "  async def on_start(self, context : RunContextWrapper , agent : Agent):\n",
        "    print(f\"Display Name {self.display_name} :: Counter {self.event_counter} Usage :: {context.usage}\")\n",
        "    self.event_counter += 1\n",
        "\n",
        "  async def on_end(self,context:RunContextWrapper,agent:Agent,final_output:str)->None:\n",
        "    print(f\"Display Name {self.display_name} :: Counter {self.event_counter} Usage :: {context.usage} Final :: {final_output}\")\n",
        "    self.event_counter += 1\n",
        "\n",
        "  async def on_handoff(self,context:RunContextWrapper,agent:Agent,source:Agent)->None:\n",
        "    print(f\"Display Name {self.display_name} :: Counter {self.event_counter} Usage :: {context.usage} Handoff To :: {source.name}\")\n",
        "\n",
        "  async def on_tool_start(self, context: RunContextWrapper, agent: Agent, tool: Tool) -> None:\n",
        "     print(f\"Display Name {self.display_name} :: Counter {self.event_counter} Usage :: {context.usage} Tool :: {tool.name}\")\n",
        "     self.event_counter += 1\n",
        "\n",
        "  async def on_tool_end(self, context: RunContextWrapper, agent: Agent, tool: Tool, result: str) -> None:\n",
        "    print(f\"Display Name  {self.display_name} :: Counter {self.event_counter} Usage :: {context.usage} Tool :: {tool.name} Result :: {result}\")\n",
        "    self.event_counter += 1\n",
        "\n",
        "@function_tool\n",
        "def multiply(a:int,b:int) -> int:\n",
        "  \"\"\"Multiplies two numbers\"\"\"\n",
        "  return a * b\n",
        "\n",
        "@function_tool\n",
        "def random_number(min:int,max:int) -> int:\n",
        "  \"\"\"Returns a random number\"\"\"\n",
        "  return random.randint(min,max)\n",
        "\n",
        "multiply_agent = Agent(\n",
        "    name = \"Multiply Agent\",\n",
        "    instructions= \"You are a helpful assistant.\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    tools = [multiply],\n",
        "    hooks = AgentWithHooks(display_name=\"Multiply Agent\"),\n",
        "    handoff_description=\"Multiply Agent Specialized in Multiplication\"\n",
        ")\n",
        "\n",
        "main_agent = Agent(\n",
        "    name = \"Main Agent\",\n",
        "    instructions= \"You are a helpful assistant.\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    tools = [random_number],\n",
        "    handoffs = [multiply_agent],\n",
        "    hooks = AgentWithHooks(display_name=\"Main Agent\"),\n",
        ")\n",
        "result_with_hooks = Runner.run_sync(\n",
        "    starting_agent=main_agent,\n",
        "    input=\"Multiply 5 and 10\",\n",
        ")\n",
        "print(f\"Hooks : {result_with_hooks.final_output}\")\n",
        "print(\"=======\" * 20)\n",
        "\n",
        "result_with_hooks_2 = Runner.run_sync(\n",
        "    starting_agent=main_agent,\n",
        "    input=\"Generate a random number between 0 and 10\",\n",
        ")\n",
        "print(f\"Hooks : {result_with_hooks_2.final_output}\")\n",
        "print(\"=======\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yYKoB0Vf7az",
        "outputId": "de0c6281-2eef-4280-8be7-c95e17d9b153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Display Name Main Agent :: Counter 0 Usage :: Usage(requests=0, input_tokens=0, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=0, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=0)\n",
            "Display Name Main Agent :: Counter 1 Usage :: Usage(requests=1, input_tokens=105, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=14, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=196) Handoff To :: Main Agent\n",
            "Display Name Multiply Agent :: Counter 0 Usage :: Usage(requests=1, input_tokens=105, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=14, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=196)\n",
            "Display Name Multiply Agent :: Counter 1 Usage :: Usage(requests=2, input_tokens=206, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=33, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=368) Tool :: multiply\n",
            "Display Name  Multiply Agent :: Counter 2 Usage :: Usage(requests=2, input_tokens=206, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=33, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=368) Tool :: multiply Result :: 50\n",
            "Display Name Multiply Agent :: Counter 3 Usage :: Usage(requests=3, input_tokens=341, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=44, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=571) Final :: 5 multiplied by 10 is 50.\n",
            "Hooks : 5 multiplied by 10 is 50.\n",
            "============================================================================================================================================\n",
            "Display Name Main Agent :: Counter 1 Usage :: Usage(requests=0, input_tokens=0, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=0, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=0)\n",
            "Display Name Main Agent :: Counter 2 Usage :: Usage(requests=1, input_tokens=109, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=21, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=248) Tool :: random_number\n",
            "Display Name  Main Agent :: Counter 3 Usage :: Usage(requests=1, input_tokens=109, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=21, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=248) Tool :: random_number Result :: 1\n",
            "Display Name Main Agent :: Counter 4 Usage :: Usage(requests=2, input_tokens=255, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=28, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=401) Final :: The random number is 1.\n",
            "Hooks : The random number is 1.\n",
            "============================================================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hooks can change our context\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, List\n",
        "\n",
        "from agents import Agent, Runner, AgentHooks, function_tool,ToolsToFinalOutputResult\n",
        "\n",
        "@dataclass\n",
        "class MyContext:\n",
        "    name: str\n",
        "    tool_calls: int = 0  # we‚Äôll increment this in our hook\n",
        "\n",
        "@function_tool\n",
        "def echo_tool(ctx: MyContext, message: str) -> str:\n",
        "    return f\"{ctx.name} says: {message}\"\n",
        "\n",
        "class CounterHook(AgentHooks[MyContext]):\n",
        "    async def on_tool_start(\n",
        "        self,\n",
        "        context,\n",
        "        agent,\n",
        "        tool,\n",
        "    ) -> None:\n",
        "      print(f\"üîî Tool '{tool.name}' started. Total calls so far: {context.context.tool_calls}\")\n",
        "    async def on_tool_end(\n",
        "        self,\n",
        "        context,\n",
        "        agent,\n",
        "        tool,\n",
        "        result: Any\n",
        "    ) -> None:\n",
        "        context.context.tool_calls += 1\n",
        "        print(f\"üîî Tool '{tool.name}' finished. Total calls so far: {context.context.tool_calls}\")\n",
        "\n",
        "def stop_after_one(\n",
        "    context,\n",
        "    tool_results: List[Any]\n",
        ") -> ToolsToFinalOutputResult:\n",
        "    for tr in tool_results:\n",
        "      if tr.tool.name == \"echo_tool\":\n",
        "        return ToolsToFinalOutputResult(is_final_output=True, final_output=tr.output)\n",
        "    # as soon as any tool runs, stop\n",
        "    return ToolsToFinalOutputResult(is_final_output=True, final_output=tool_results[-1].output)\n",
        "\n",
        "agent = Agent[MyContext](\n",
        "    name=\"CounterAgent\",\n",
        "    instructions=\"Echo back the user‚Äôs message using echo_tool, then stop.\",\n",
        "    tools=[echo_tool],\n",
        "    tool_use_behavior=stop_after_one,\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    hooks = CounterHook()\n",
        ")\n",
        "\n",
        "result = Runner.run_sync(\n",
        "    starting_agent=agent,\n",
        "    input=\"Hello, world!\",\n",
        "    context=MyContext(name=\"Saim\"),\n",
        "    # attach our hook here\n",
        ")\n",
        "\n",
        "print(\"üèÅ Final Output:\", result.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsloEv4-0S3L",
        "outputId": "64926436-4db4-4d60-b627-66ac3d50ea66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîî Tool 'echo_tool' started. Total calls so far: 0\n",
            "üîî Tool 'echo_tool' finished. Total calls so far: 1\n",
            "üèÅ Final Output: echo_tool says: Hello, world!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# input_guardrails"
      ],
      "metadata": {
        "id": "Kcarx9iI6Xg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input_guardrails\n",
        "from agents import Agent,Runner,GuardrailFunctionOutput,RunContextWrapper,InputGuardrail,InputGuardrailTripwireTriggered,input_guardrail\n",
        "from pydantic import BaseModel,Field\n",
        "\n",
        "class HomeWork(BaseModel):\n",
        "  reasoning : str = Field(description=\"Reasoning about whether the user input is related to homework or not\")\n",
        "  is_homework : bool\n",
        "\n",
        "homework_guardrail_agent = Agent(\n",
        "    name = \"Homework Agent\",\n",
        "    instructions= \"You are a helpful assistant.Check if the user is asking about HomeWork\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    output_type = HomeWork\n",
        ")\n",
        "\n",
        "class Department(BaseModel):\n",
        "  is_IT_department : bool\n",
        "  is_HR_department : bool\n",
        "\n",
        "department_guardrail_agent = Agent(\n",
        "    name = \"Department Agent\",\n",
        "    instructions= \"You are a helpful assistant.Check if the user asking is from IT or HR department\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    output_type = Department\n",
        ")\n",
        "\n",
        "def input_guardrails_1(ctx:RunContextWrapper,agent:Agent,input_text:str):\n",
        "  input_guardrails_1_result = Runner.run_sync(\n",
        "      starting_agent=homework_guardrail_agent,\n",
        "      input=input_text,\n",
        "      context = ctx.context\n",
        "  )\n",
        "  final_output_1 = input_guardrails_1_result.final_output_as(HomeWork)\n",
        "  return GuardrailFunctionOutput(\n",
        "      output_info=final_output_1,\n",
        "      tripwire_triggered= not final_output_1.is_homework\n",
        "  )\n",
        "\n",
        "@input_guardrail\n",
        "async def input_guardrails_2(ctx:RunContextWrapper,agent:Agent,input_text:str):\n",
        "  input_guardrails_2_result = await Runner.run(\n",
        "      starting_agent=department_guardrail_agent,\n",
        "      input=input_text,\n",
        "      context = ctx.context\n",
        "  )\n",
        "  final_output_2 = input_guardrails_2_result.final_output_as(Department)\n",
        "  return GuardrailFunctionOutput(\n",
        "      output_info=final_output_2,\n",
        "      tripwire_triggered= not final_output_2.is_IT_department\n",
        "  )\n",
        "\n",
        "agent_with_input_guardrails = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    instructions= \"You are a helpful assistant.\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    input_guardrails = [InputGuardrail(guardrail_function=input_guardrails_1),\n",
        "                        input_guardrails_2]\n",
        ")\n",
        "try:\n",
        " result_with_input_guardrails = Runner.run_sync(\n",
        "    starting_agent=agent_with_input_guardrails,\n",
        "    input=\"What is conversational ai in 100 words and I am from IT department\",\n",
        ")\n",
        " print(f\"Agent Input Guardrails : {result_with_input_guardrails.final_output}\")\n",
        " print(\"=======\" * 20)\n",
        "except InputGuardrailTripwireTriggered as e:\n",
        "  print(f\"Input Guardrails Triggered: {e.guardrail_result.output.output_info}\")\n",
        "  print(f\"Input Guardrails Triggered: {e.guardrail_result.output.tripwire_triggered}\")\n",
        "  print(f\"Sorry I can help you with only Homework Related Question if you are from IT Department Only\")\n",
        "  print(\"=======\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMa5-zZf6a0V",
        "outputId": "7adf6d01-ff52-4962-bda7-2c8390f06b81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Guardrails Triggered: reasoning=\"The user is asking for a definition of a technical concept ('conversational AI') and provides context about their professional department ('IT department'). This is a request for information and knowledge, not a typical homework assignment that would involve problem-solving, essay writing, or specific academic tasks.\" is_homework=False\n",
            "Input Guardrails Triggered: True\n",
            "Sorry I can help you with only Homework Related Question if you are from IT Department Only\n",
            "============================================================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# output_guardrails"
      ],
      "metadata": {
        "id": "9Kw3KgCAYtad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# output_guardrails\n",
        "from agents import Agent,Runner,GuardrailFunctionOutput,RunContextWrapper,OutputGuardrail,OutputGuardrailTripwireTriggered,output_guardrail\n",
        "from pydantic import BaseModel,Field\n",
        "\n",
        "class MessageOutput(BaseModel):\n",
        "  reasoning : str = Field(description=\"The reason behind your response\")\n",
        "  response : str = Field(description=\"The actual response to the user\")\n",
        "  user_name : str = Field(description=\"The name of the user\")\n",
        "\n",
        "name_checker_guardrail_agent = Agent(\n",
        "    name = \"Output Guardrail Agent\",\n",
        "    instructions= \"You are a helpful assistant.Check if the user name is included in the response\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    output_type = MessageOutput\n",
        ")\n",
        "\n",
        "def output_guardrails_1(ctx:RunContextWrapper,agent:Agent,output:MessageOutput)->GuardrailFunctionOutput:\n",
        "  phone_num_in_response = \"0300\" in output.response\n",
        "  phone_num_in_reasoning = \"0300\" in output.reasoning\n",
        "  return GuardrailFunctionOutput(\n",
        "      output_info = {\n",
        "          \"is_phone_number_in_response\" : phone_num_in_response,\n",
        "          \"is_phone_number_in_reasoning\" : phone_num_in_reasoning\n",
        "      },\n",
        "      tripwire_triggered= phone_num_in_response or phone_num_in_reasoning\n",
        "  )\n",
        "\n",
        "@output_guardrail\n",
        "async def output_guardrails_2(ctx:RunContextWrapper,agent:Agent,output:MessageOutput):\n",
        "  output_guardrails_2_result = await Runner.run(\n",
        "      starting_agent=name_checker_guardrail_agent,\n",
        "      input=output.response,\n",
        "      context=ctx.context\n",
        "  )\n",
        "  # final_output_2 = output_guardrails_2_result.final_output_as(MessageOutput)\n",
        "  final_output_2 = output_guardrails_2_result.final_output\n",
        "  return GuardrailFunctionOutput(\n",
        "      output_info=final_output_2,\n",
        "      tripwire_triggered = final_output_2.user_name\n",
        "  )\n",
        "\n",
        "agent_with_output_guardrails = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    instructions= \"You are a helpful assistant.\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    output_guardrails = [OutputGuardrail(guardrail_function=output_guardrails_1),\n",
        "                        output_guardrails_2],\n",
        "    output_type = MessageOutput\n",
        ")\n",
        "try:\n",
        "  result_with_output_guardrails = Runner.run_sync(\n",
        "      starting_agent=agent_with_output_guardrails,\n",
        "      input=\"My name is 'Saim Hassan',reply me with my name\" # Try with phone number as input as well to check the other Output Guardrail\n",
        "  )\n",
        "  print(f\"Agent Output Guardrails : {result_with_output_guardrails.final_output}\")\n",
        "  print(\"=======\" * 20)\n",
        "except OutputGuardrailTripwireTriggered as e:\n",
        "  print(f\"Output Guardrails Triggered : {e.guardrail_result.output.output_info}\")\n",
        "  print(f\"Output Guardrails Triggered : {e.guardrail_result.output.tripwire_triggered}\")\n",
        "  print(f\"Sorry As your request contains Sensitive data so I cannot approve it\")\n",
        "  print(\"=======\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86bbb612-d151-4fb3-f0fe-f77798573eb7",
        "id": "hixDlaISZPAI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output Guardrails Triggered : reasoning=\"The user provided their name 'Saim Hassan' in the greeting. I am acknowledging the greeting and including the extracted name in the 'user_name' field.\" response=\"Hello Saim Hassan! It's good to hear from you.\" user_name='Saim Hassan'\n",
            "Output Guardrails Triggered : Saim Hassan\n",
            "Sorry As your request contains Sensitive data so I cannot approve it\n",
            "============================================================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tool_use_behavior"
      ],
      "metadata": {
        "id": "RaKfW_chxm8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ToolsToFinalOutputFunction\n",
        "from typing_extensions import final\n",
        "from agents import (\n",
        "    ToolsToFinalOutputResult,\n",
        "    ToolsToFinalOutputFunction,\n",
        "    function_tool,\n",
        "    RunContextWrapper,\n",
        ")\n",
        "from pydantic import BaseModel\n",
        "from typing import Any\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@function_tool\n",
        "def reverse_tool(context: RunContextWrapper, text: str) -> str:\n",
        "    \"\"\"Transforms input text uniquely.\"\"\"\n",
        "    return f\"SPECIAL REVERSE üöó : {text[::-1]}\"\n",
        "\n",
        "def toolfuncoutput(context : RunContextWrapper,tool_results :list[Any]) -> ToolsToFinalOutputResult :\n",
        "  for tr in tool_results:\n",
        "    if tr.tool.name == \"reverse_tool\":\n",
        "      return ToolsToFinalOutputResult(\n",
        "          is_final_output = True,\n",
        "          final_output = tr.output\n",
        "      )\n",
        "  return ToolsToFinalOutputResult(is_final_output=False)\n",
        "\n",
        "agent = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    instructions=     \"You are a helpful assistant.\\n\"\n",
        "    \"Use reverse_tool to reverse text when appropriate.\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    tools = [reverse_tool],\n",
        "    tool_use_behavior=toolfuncoutput\n",
        ")\n",
        "\n",
        "result = Runner.run_sync(\n",
        "    starting_agent=agent,\n",
        "    input=\" Reverse these letters 'My Name is Saim, How Are You'\",\n",
        ")\n",
        "print(f\"ToolsToFinalOutputFunction : {result.final_output}\")\n",
        "print(\"========\"*20)\n",
        "\n",
        "# stop_on_first_tool\n",
        "@function_tool\n",
        "def add( a: int, b: int) -> int:\n",
        "    \"\"\"Adds two numbers.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "@function_tool\n",
        "def subtract( a: int, b: int) -> int:\n",
        "    \"\"\"Subtracts two numbers.\"\"\"\n",
        "    return a - b\n",
        "\n",
        "agent1 = Agent(\n",
        "    name = \"Calculator Agent\",\n",
        "    instructions= \"You are a special agent that helps with calculations\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    tools = [add,subtract],\n",
        "    tool_use_behavior = \"stop_on_first_tool\"\n",
        ")\n",
        "\n",
        "result1 = Runner.run_sync(\n",
        "    starting_agent=agent1,\n",
        "    input=\"Add 5 and 10\",\n",
        ")\n",
        "print(f\"Stop On First Tool : {result1.final_output}\")\n",
        "print(\"========\"*20)\n",
        "\n",
        "# StopAtTools\n",
        "@function_tool\n",
        "def add( a: int, b: int) -> int:\n",
        "    \"\"\"Adds two numbers.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "@function_tool\n",
        "def subtract( a: int, b: int) -> int:\n",
        "    \"\"\"Subtracts two numbers.\"\"\"\n",
        "    return a - b\n",
        "\n",
        "agent2 = Agent(\n",
        "    name = \"Calculator Agent\",\n",
        "    instructions= \"You are a special agent that helps with calculations\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    tools = [add,subtract],\n",
        "    tool_use_behavior = {\n",
        "        \"stop_at_tool_names\" : [\"add\",\"subtract\"]\n",
        "    }\n",
        ")\n",
        "\n",
        "result2 = Runner.run_sync(\n",
        "    starting_agent=agent2,\n",
        "    input=\"subtract 10 from 100 and add 20 and 30\",\n",
        ")\n",
        "print(f\"StopAtTools : {result2.final_output}\")\n",
        "print(\"========\"*20)\n",
        "# Will stop on first tool call name \"add\"\n",
        "\n",
        "# run_llm_again\n",
        "@function_tool\n",
        "def add( a: int, b: int) -> int:\n",
        "    \"\"\"Adds two numbers.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "@function_tool\n",
        "def subtract( a: int, b: int) -> int:\n",
        "    \"\"\"Subtracts two numbers.\"\"\"\n",
        "    return a - b\n",
        "\n",
        "agent3 = Agent(\n",
        "    name = \"Calculator Agent\",\n",
        "    instructions= \"You are a special agent that helps with calculations\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        "    tools = [add,subtract],\n",
        "    tool_use_behavior = \"run_llm_again\"\n",
        ")\n",
        "\n",
        "result3 = Runner.run_sync(\n",
        "    starting_agent=agent3,\n",
        "    input=\"subtract 10 from 100 and add 20 and 30\",\n",
        ")\n",
        "print(f\"Run LLm Again : {result3.final_output}\")\n",
        "print(\"========\"*20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_czZ-Ff-Tm5Z",
        "outputId": "f79e73ed-8bbc-4fe6-9580-b587598a69e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ToolsToFinalOutputFunction : SPECIAL REVERSE üöó : uoY erA woH ,miaS si emaN yM\n",
            "================================================================================================================================================================\n",
            "Stop On First Tool : 15\n",
            "================================================================================================================================================================\n",
            "StopAtTools : 90\n",
            "================================================================================================================================================================\n",
            "Run LLm Again : The result of subtracting 10 from 100 is 90. The result of adding 20 and 30 is 50.\n",
            "================================================================================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# reset_tool_choice\n",
        "**reset_tool_choice is a bool parameter that is default to True so if there are changes in 'tool_choice' in ModelSettings so to reset it to 'auto' again to prevent the infinite loop after every turn.**"
      ],
      "metadata": {
        "id": "r-m-ZjkgqSgD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# output_type"
      ],
      "metadata": {
        "id": "o5oiHRz_rFr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1st method for output_type\n",
        "from agents import Agent,Runner\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class OutputType(BaseModel):\n",
        "  reasoning : str\n",
        "  response :  str\n",
        "\n",
        "agent1_with_output_type = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    instructions= \"You are a helpful assistant.\",\n",
        "    model = \"gemini-2.5-pro\",\n",
        "    output_type = OutputType\n",
        ")\n",
        "result1_with_output_type = await Runner.run(\n",
        "    starting_agent=agent1_with_output_type,\n",
        "    input=\"What is the future of AI in 100 words\",\n",
        ")\n",
        "print(f\"Output Type : {result1_with_output_type.final_output}\")\n",
        "print(f\"Output Type Reasoning: {result1_with_output_type.final_output.reasoning}\")\n",
        "print(f\"Output Type Response: {result1_with_output_type.final_output.response}\")\n",
        "print(\"=======\" * 20)"
      ],
      "metadata": {
        "id": "onMjj9El3i7D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f56727f2-ff23-495c-b6aa-15072e5ff400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output Type : reasoning=\"The user requested a 100-word summary about the future of AI. I will synthesize key future trends, including deeper integration into daily life, its role as a collaborative tool for complex problem-solving, economic impacts, and the associated ethical challenges. I will ensure the response is concise and within the specified word count, presenting a balanced view of AI's potential and the necessary considerations for its development.\" response='The future of AI points towards its seamless integration into the fabric of daily life, from autonomous transportation and hyper-personalized healthcare to dynamic, AI-driven education and entertainment. It will evolve from a tool into a collaborative partner, augmenting human intellect to tackle complex global challenges like climate change and disease. While the pursuit of Artificial General Intelligence (AGI) continues, the economic landscape will be reshaped by AI-driven automation and new job creation. Navigating this future successfully will hinge on developing robust ethical frameworks to ensure fairness, transparency, and alignment with human values.'\n",
            "Output Type Reasoning: The user requested a 100-word summary about the future of AI. I will synthesize key future trends, including deeper integration into daily life, its role as a collaborative tool for complex problem-solving, economic impacts, and the associated ethical challenges. I will ensure the response is concise and within the specified word count, presenting a balanced view of AI's potential and the necessary considerations for its development.\n",
            "Output Type Response: The future of AI points towards its seamless integration into the fabric of daily life, from autonomous transportation and hyper-personalized healthcare to dynamic, AI-driven education and entertainment. It will evolve from a tool into a collaborative partner, augmenting human intellect to tackle complex global challenges like climate change and disease. While the pursuit of Artificial General Intelligence (AGI) continues, the economic landscape will be reshaped by AI-driven automation and new job creation. Navigating this future successfully will hinge on developing robust ethical frameworks to ensure fairness, transparency, and alignment with human values.\n",
            "============================================================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2nd method of output_type\n",
        "from agents import Agent,Runner,AgentOutputSchema\n",
        "from pydantic import BaseModel,Field\n",
        "\n",
        "class OutputType2(BaseModel):\n",
        "  response : str\n",
        "  is_homework : bool\n",
        "  is_good_question : bool\n",
        "\n",
        "agent2_with_output_type = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    instructions= \"You are a helpful assistant.\",\n",
        "    model = \"gemini-2.5-pro\",\n",
        "    output_type = AgentOutputSchema(output_type=OutputType2,strict_json_schema=False)\n",
        ")\n",
        "result2_with_output_type = await Runner.run(\n",
        "    starting_agent=agent2_with_output_type,\n",
        "    input=\"What is cloud computing in 100 words\",\n",
        ")\n",
        "print(f\"Output Type : {result2_with_output_type.final_output}\")\n",
        "print(f\"Output Type Response: {result2_with_output_type.final_output.response}\")\n",
        "print(f\"Output Type is_homework: {result2_with_output_type.final_output.is_homework}\")\n",
        "print(f\"Output Type is_good_question: {result2_with_output_type.final_output.is_good_question}\")\n",
        "print(\"=======\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVFH0U9it47X",
        "outputId": "2d6909d2-9a69-4d1c-cb69-a49c272fb412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output Type : response='Cloud computing is the on-demand delivery of IT resources and services over the internet with pay-as-you-go pricing. Instead of buying, owning, and maintaining your own physical data centers and servers, you can access technology services from a cloud provider like Amazon Web Services (AWS) or Microsoft Azure. These services include computing power, storage, databases, networking, and software. This model offers faster innovation, flexible resources, and economies of scale, allowing businesses to scale operations efficiently without large upfront capital investments. It empowers users to access data and applications from virtually any device with an internet connection.' is_homework=True is_good_question=True\n",
            "Output Type Response: Cloud computing is the on-demand delivery of IT resources and services over the internet with pay-as-you-go pricing. Instead of buying, owning, and maintaining your own physical data centers and servers, you can access technology services from a cloud provider like Amazon Web Services (AWS) or Microsoft Azure. These services include computing power, storage, databases, networking, and software. This model offers faster innovation, flexible resources, and economies of scale, allowing businesses to scale operations efficiently without large upfront capital investments. It empowers users to access data and applications from virtually any device with an internet connection.\n",
            "Output Type is_homework: True\n",
            "Output Type is_good_question: True\n",
            "============================================================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent Class\n",
        "**Agent Class Contains Following Methods**\n",
        "\n",
        "1. **clone** :: For the cloning the agent with overriding desired parameters\n",
        "2. **as_tool** :: To use an agent as a function_tool for another agent\n",
        "3. **get_system_prompt** :: To get the system prompt for the agent\n",
        "4. **get_prompt** :: To get the prompt for the agent\n",
        "5. **get_mcp_tools** :: To get the lsit of MCP server tools available to the agent\n",
        "6. **get_all_tools** :: To get the list of all the tools avilable to the agent including both FunctionTools and Hosted MCP tools."
      ],
      "metadata": {
        "id": "jPIPojYy3Syd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# clone()"
      ],
      "metadata": {
        "id": "jbkdjeKY5vMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clone\n",
        "from agents import Agent\n",
        "\n",
        "agent_1 = Agent(\n",
        "    name = \"Assistant Agent\",\n",
        "    instructions= \"You are a helpful assistant.\",\n",
        "    model = \"gemini-2.5-flash\",\n",
        ")\n",
        "agent_2 = agent_1.clone(\n",
        "    name = \"Clone Agent\",\n",
        "    instructions = \"You are a helpful Clone Assistant\",\n",
        "    model = \"gemini-2.5-pro\"\n",
        ")\n",
        "agent_3 = agent_2.clone(\n",
        "    name = \"Clone Agent 2\",\n",
        "    instructions = \"You are a helpful Clone Assistant 2\",\n",
        ")\n",
        "\n",
        "print(f\"Agent 1 : {agent_1.name}\")\n",
        "print(f\"Agent 2 : {agent_2.name}\")\n",
        "print(f\"Agent 3 : {agent_3.name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kg4m4FHS402x",
        "outputId": "ad881eda-e7b5-4cd8-cac1-eeae472c599f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent 1 : Assistant Agent\n",
            "Agent 2 : Clone Agent\n",
            "Agent 3 : Clone Agent 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# as_tool()"
      ],
      "metadata": {
        "id": "GMniTyMp5ydh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent,Runner\n",
        "\n",
        "agent_summarization = Agent(\n",
        "    name = \"Summarization Agent\",\n",
        "    instructions=\"You are a summarization agent that makes a summary of the given topic in 100 words\",\n",
        "    model=\"gemini-2.5-pro\",\n",
        ")\n",
        "\n",
        "main_agent = Agent(\n",
        "    name = \"Main Agent\",\n",
        "    instructions=\"Reply with user queries\",\n",
        "    tools = [agent_summarization.as_tool(\n",
        "        tool_name=\"Summarization_Tool\",\n",
        "        tool_description=\"Summarization Tool that summarizes the topic into 100 words\",\n",
        "    )],\n",
        "    model = \"gemini-2.5-flash\"\n",
        ")\n",
        "result_with_as_tool = await Runner.run(\n",
        "    starting_agent=main_agent,\n",
        "    input=\"Write a summary of the 'Machine Learning'\"\n",
        ")\n",
        "print(result_with_as_tool.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtuXJ6oW5fbz",
        "outputId": "e9b90298-45dd-4e13-ad9f-46628373fd5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine Learning (ML) is a subfield of Artificial Intelligence that enables systems to learn and improve from experience without being explicitly programmed. Core to ML are algorithms that parse data, identify patterns, and build models to make predictions or decisions. The primary types are supervised learning (using labeled data), unsupervised learning (finding hidden structures in unlabeled data), and reinforcement learning (learning through rewards and penalties). This technology powers countless applications, including recommendation engines, natural language processing, medical diagnostics, and autonomous vehicles, transforming industries by extracting valuable insights and automating complex tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_tool = agent_summarization.as_tool(\n",
        "    tool_name=None,\n",
        "    tool_description=None,\n",
        ")\n",
        "print(agent_tool)\n",
        "print(agent_tool.name)\n",
        "print(f\"The description is {agent_tool.description}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx7l8xcmAknv",
        "outputId": "c35ddf8e-3f2c-4577-a424-d451185007bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FunctionTool(name='summarization_agent', description='', params_json_schema={'properties': {'input': {'title': 'Input', 'type': 'string'}}, 'required': ['input'], 'title': 'summarization_agent_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7d5fd0a60b80>, strict_json_schema=True, is_enabled=True)\n",
            "summarization_agent\n",
            "The description is \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# as_tool() with custom_output_extractor\n",
        "agent_tool = agent_summarization_2.as_tool(\n",
        "    tool_name=\"Summarization_Tool\",\n",
        "    tool_description=\"Summarization Tool that summarizes the topic in 5 line each line with an emoji\",\n",
        ")\n",
        "print(agent_tool)\n",
        "print(agent_tool.name)\n",
        "print(f\"The description is {agent_tool.description}\")\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class CustomOutput(BaseModel):\n",
        "  summary : str\n",
        "  reasoning : str\n",
        "  is_good_response : bool\n",
        "\n",
        "agent_summarization_2 = Agent(\n",
        "    name = \"Summarization Agent\",\n",
        "    instructions=\"You are a summarization agent that makes a summary of the given topic in 5 line each line with an emoji\",\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    output_type = CustomOutput\n",
        ")\n",
        "\n",
        "async def custom_output_extractor(run_result):\n",
        "  get_output = run_result.final_output_as(CustomOutput)\n",
        "  return get_output.summary\n",
        "\n",
        "main_agent_2 = Agent(\n",
        "    name = \"Main Agent\",\n",
        "    instructions=\"Reply with user queries\",\n",
        "    tools = [agent_summarization_2.as_tool(\n",
        "        tool_name=\"Summarization_Tool\",\n",
        "        tool_description=\"Summarization Tool that summarizes the topic in 5 line each line with an emoji\",\n",
        "        custom_output_extractor=custom_output_extractor)],\n",
        "    model = \"gemini-2.5-flash\"\n",
        ")\n",
        "result_with_as_tool_2 = await Runner.run(\n",
        "    starting_agent=main_agent_2,\n",
        "    input=\"Write a summary of the 'Deep Learning'\"\n",
        ")\n",
        "print(result_with_as_tool_2.final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJAEndbwBadv",
        "outputId": "d9592c02-032c-44e3-e483-a1f3d24a27b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FunctionTool(name='Summarization_Tool', description='Summarization Tool that summarizes the topic in 5 line each line with an emoji', params_json_schema={'properties': {'input': {'title': 'Input', 'type': 'string'}}, 'required': ['input'], 'title': 'Summarization_Tool_args', 'type': 'object', 'additionalProperties': False}, on_invoke_tool=<function function_tool.<locals>._create_function_tool.<locals>._on_invoke_tool at 0x7d5fd0a63740>, strict_json_schema=True, is_enabled=True)\n",
            "Summarization_Tool\n",
            "The description is Summarization Tool that summarizes the topic in 5 line each line with an emoji\n",
            "Deep Learning is a subset of Machine Learning. üß†It uses artificial neural networks with multiple layers. üèóÔ∏èThese networks learn complex patterns from vast amounts of data. üìàIt excels in tasks like image recognition and natural language processing. ü§ñDL models automatically discover features, making them powerful for predictions. ‚ú®\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# get_system_prompt"
      ],
      "metadata": {
        "id": "pPLroXkSjFPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent,Runner,RunContextWrapper\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# static_prompt\n",
        "agent_with_system_prompt = Agent(\n",
        "    name = \"Summarization Agent\",\n",
        "    instructions=\"You are a summarization agent that makes a summary of the given topic in 100 words\",\n",
        "    model=\"gemini-2.5-pro\",\n",
        ")\n",
        "system_prompt = await agent_with_system_prompt.get_system_prompt(run_context=None)\n",
        "print(system_prompt)\n",
        "\n",
        "# dynamic_prompt\n",
        "class Userdata(BaseModel):\n",
        "  name : str\n",
        "  age : int\n",
        "\n",
        "async def system_prompt(context:RunContextWrapper[Userdata],agent:Agent):\n",
        "  return f\"Your name is {context.context.name} and you are {context.context.age} years old\"\n",
        "\n",
        "agent_with_system_prompt_2 = Agent(\n",
        "    name = \"Info Agent\",\n",
        "    instructions=system_prompt,\n",
        "    model=\"gemini-2.5-pro\",\n",
        ")\n",
        "run_context = RunContextWrapper(context=Userdata(name=\"Saim Hassan\",age=21))\n",
        "system_prompt_2 = await agent_with_system_prompt_2.get_system_prompt(run_context)\n",
        "print(system_prompt_2)\n"
      ],
      "metadata": {
        "id": "cgGPndvoFjdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "732baf8b-c25c-4f50-c059-47b84f5343ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a summarization agent that makes a summary of the given topic in 100 words\n",
            "Your name is Saim Hassan and you are 21 years old\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# get_prompt , get_mcp_tools, get_all_tools"
      ],
      "metadata": {
        "id": "RYkRkL44p_f3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result =  await agent_with_system_prompt.get_prompt(run_context=run_context)\n",
        "print(result)\n",
        "result_2 = await agent_with_system_prompt.get_mcp_tools(run_context=None)\n",
        "print(result_2)\n",
        "result_3 = await agent_with_system_prompt.get_all_tools(run_context=None)\n",
        "print(result_3)\n",
        "# all of this attributes are None or Empty in Case Of Our Agent.Above are example cases of how to use the"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcWMvgdujVql",
        "outputId": "01927ebc-42b3-4254-a98a-2a155179e879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "[]\n",
            "[]\n"
          ]
        }
      ]
    }
  ]
}